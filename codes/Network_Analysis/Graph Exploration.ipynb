{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project: North Macedonia Procurment Network Analysis (2011-2022)  \n",
    "Date: 30/1/2023  \n",
    "Author: Aly Abdou   \n",
    "Dataset: MK_202212_processed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# import pymaterial\n",
    "import scipy.stats as stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create required directories\n",
    "os.makedirs('data', exist_ok=True)\n",
    "# Copy MK_202212_processed_network_exploration.csv.gz to this data folder\n",
    "os.makedirs('data/graphs', exist_ok=True)\n",
    "os.makedirs('data/edgelists', exist_ok=True)\n",
    "os.makedirs('data/figures', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "pp_data = pd.read_csv(f'data/MK_202212_processed_network_exploration.csv.gz', encoding=\"utf-8\",\n",
    "                      dtype={'buyer_id': str, 'bidder_id': str}, compression='gzip')\n",
    "\n",
    "# pp_data = pp_data.rename(columns={'tender_cpvs_original': 'tender_cpvs'})\n",
    "\n",
    "# Variable addition and selection\n",
    "# Filter variables\n",
    "cols = ['tender_id','buyer_id','buyer_name','bidder_name','bidder_id','year','month','bid_price',\n",
    "        'tender_cpvs','tender_supplytype','buyer_nuts','cri','filter_ok']\n",
    "pp_data = pp_data[cols]\n",
    "\n",
    "\n",
    "# Create market \n",
    "pp_data['market'] = pp_data['tender_cpvs'].str[:2]\n",
    "\n",
    "\n",
    "pp_data['buyer_loc'] = pp_data['buyer_nuts'].replace({\n",
    "    'MK001': 'Vardarski',\n",
    "    'MK002': 'Istočen',\n",
    "    'MK003': 'Jugozapaden',\n",
    "    'MK004': 'Jugoistočen',\n",
    "    'MK005': 'Pelagoniski',\n",
    "    'MK006': 'Pološki',\n",
    "    'MK007': 'Severoistočen',\n",
    "    'MK008': 'Skopski',\n",
    "})\n",
    "\n",
    "#Apply filter_ok\n",
    "pp_data = pp_data[pp_data[\"filter_ok\"] == 1]\n",
    "\n",
    "# Drop missing buyer id and buyer_nuts\n",
    "pp_data = pp_data.dropna(subset=['buyer_id'])\n",
    "pp_data = pp_data.dropna(subset=['buyer_nuts'])\n",
    "pp_data = pp_data.dropna(subset=['year'])\n",
    "\n",
    "# Replace node id to B_ and S_\n",
    "pp_data[\"buyer_id\"] = \"B_\"+ pp_data[\"buyer_id\"] \n",
    "pp_data[\"bidder_id\"] = \"S_\"+ pp_data[\"bidder_id\"] \n",
    "\n",
    "pp_data['year'] = pp_data['year'].round().astype(int).astype(str)\n",
    "\n",
    "\n",
    "# pp_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_markets = pp_data[['market','bid_price']]\n",
    "pp_markets['totals'] = pp_markets.groupby('market')['bid_price'].transform('sum')\n",
    "pp_markets = pp_markets.drop_duplicates(['market','totals'], keep='first')\n",
    "pp_markets = pp_markets[['market','totals']]\n",
    "pp_markets.sort_values('totals')\n",
    "pp_markets\n",
    "# pp_markets.groupby(['totals']).size().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_data['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_data['market'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp_data = pp_data.sample(10000)\n",
    "# pp_data_const = pp_data[pp_data['market'] == '45']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df):\n",
    "    df_wip = df\n",
    "    # Genrate total contract counts\n",
    "    df_wip = df_wip.drop(df_wip.filter(regex='_contracts.*').columns, axis=1)\n",
    "    df_wip = df_wip.merge(df_wip.groupby('buyer_id').size().reset_index(name='buyer_contracts'),\n",
    "                        on='buyer_id', how='left')\n",
    "    df_wip = df_wip.merge(df_wip.groupby('bidder_id').size().reset_index(name='bidder_contracts'),\n",
    "                        on='bidder_id', how='left')\n",
    "    df_wip[\"edge_id\"] = df_wip['buyer_id'] +\"-\"+ df_wip[\"bidder_id\"]\n",
    "    df_wip = df_wip.merge(df_wip.groupby('edge_id').size().reset_index(name='edge_contracts'),\n",
    "                        on='edge_id', how='left')\n",
    "\n",
    "    # Generate Average CRI values\n",
    "    df_wip['buyer_cri'] = df_wip.groupby('buyer_id')['cri'].transform('mean')\n",
    "    df_wip['bidder_cri'] = df_wip.groupby('bidder_id')['cri'].transform('mean')\n",
    "    df_wip['edge_cri'] = df_wip.groupby(['buyer_id', 'bidder_id'])['cri'].transform('mean')\n",
    "\n",
    "    # Generate Average total values\n",
    "    df_wip['buyer_tot_value'] = df_wip.groupby('buyer_id')['bid_price'].transform('sum')\n",
    "    df_wip['bidder_tot_value'] = df_wip.groupby('bidder_id')['bid_price'].transform('sum')\n",
    "    df_wip['edge_tot_value'] = df_wip.groupby(['buyer_id', 'bidder_id'])['bid_price'].transform('sum')\n",
    "    \n",
    "    df_wip.reset_index()\n",
    "    # Keep only the created variables\n",
    "    keep_vars = ['buyer_id','bidder_id','year','market','tender_supplytype','buyer_loc',\n",
    "                 'buyer_contracts', 'bidder_contracts', 'edge_contracts',\n",
    "                 'buyer_cri', 'bidder_cri', 'edge_cri',\n",
    "                 'buyer_tot_value', 'bidder_tot_value', 'edge_tot_value','bid_price']\n",
    "    return df_wip[keep_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_data_processed = process_data(pp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist_vars = ['buyer_id','bidder_id','buyer_contracts', 'bidder_contracts', 'edge_contracts',\n",
    "         'buyer_cri', 'bidder_cri', 'edge_cri',\n",
    "         'buyer_tot_value', 'bidder_tot_value', 'edge_tot_value']\n",
    "edgelist = pp_data_processed[edgelist_vars]\n",
    "\n",
    "edgelist = edgelist.sort_values(['buyer_id','bidder_id']).drop_duplicates(['buyer_id','bidder_id'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edgelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_data_processed\n",
    "pp_data_processed['bid_price'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pp_data_processed['market'].unique())\n",
    "# pp_data_processed['market'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(edgelist):\n",
    "    # Create a DataFrame for the source nodes\n",
    "    buyers = edgelist[['buyer_id', 'buyer_contracts', 'buyer_cri', 'buyer_tot_value']]\n",
    "    buyers = buyers.drop_duplicates()\n",
    "    buyers = buyers.rename(columns={'buyer_id': 'node_id',\n",
    "                                    'buyer_contracts': 'contracts',\n",
    "                                    'buyer_cri': 'cri',\n",
    "                                    'buyer_tot_value': 'tot_value'})\n",
    "\n",
    "    # Create a DataFrame for the target nodes\n",
    "    bidders = edgelist[['bidder_id', 'bidder_contracts', 'bidder_cri', 'bidder_tot_value']]\n",
    "    bidders = bidders.drop_duplicates()\n",
    "    bidders = bidders.rename(columns={'bidder_id': 'node_id',\n",
    "                                      'bidder_contracts': 'contracts',\n",
    "                                      'bidder_cri': 'cri',\n",
    "                                      'bidder_tot_value': 'tot_value'})\n",
    "\n",
    "    # Concatenate the two DataFrames\n",
    "    nodes = pd.concat([buyers, bidders], ignore_index=True)\n",
    "    \n",
    "    # Create the graph\n",
    "    G = nx.from_pandas_edgelist(edgelist,\n",
    "                                source='buyer_id',\n",
    "                                target='bidder_id',\n",
    "                                edge_attr=['edge_contracts', 'edge_cri', 'edge_tot_value'],\n",
    "                                create_using=nx.Graph())\n",
    "    \n",
    "    # Add node attributes to the graph\n",
    "    nx.set_node_attributes(G, nodes.set_index('node_id').to_dict('index'))\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_statistics(G, name):\n",
    "    statistics = {}\n",
    "\n",
    "    # Graph name\n",
    "    statistics[\"Name\"] = name\n",
    "    \n",
    "    # Number of nodes\n",
    "    statistics[\"Number of nodes\"] = G.number_of_nodes()\n",
    "    \n",
    "    # Number of edges\n",
    "    statistics[\"Number of edges\"] = G.number_of_edges()\n",
    "    \n",
    "    # Average degree\n",
    "    try:\n",
    "        avg_degree = sum(d for n, d in G.degree()) / statistics[\"Number of nodes\"]\n",
    "        statistics[\"Average degree\"] = avg_degree\n",
    "    except:\n",
    "        statistics[\"Average degree\"] = ''\n",
    "    \n",
    "#     # Diameter % Radius\n",
    "#     try:\n",
    "#         statistics[\"Diameter\"] = nx.diameter(G)\n",
    "#         statistics[\"Radius\"] = nx.radius(G)\n",
    "#     except:\n",
    "#         largest_cc = max(nx.connected_components(G), key=len)\n",
    "#         largest_cc_graph = G.subgraph(largest_cc)\n",
    "#         statistics[\"Diameter\"] = nx.diameter(largest_cc_graph)\n",
    "#         statistics[\"Radius\"] = nx.radius(largest_cc_graph)\n",
    "    \n",
    "#     # Average clustering coefficient\n",
    "#     statistics[\"Average clustering coefficient\"] = nx.average_clustering(G)\n",
    "    \n",
    "#     # Transitivity\n",
    "#     statistics[\"Transitivity\"] = nx.transitivity(G)\n",
    "    \n",
    "    # Assortativity\n",
    "    statistics[\"Assortativity\"] = nx.degree_assortativity_coefficient(G)\n",
    "    \n",
    "    # Average Degree centrality\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    try:\n",
    "        avg_degree_centrality = sum(degree_centrality.values()) / len(degree_centrality)\n",
    "        statistics[\"Average degree centrality\"] = avg_degree_centrality\n",
    "    except:\n",
    "         statistics[\"Average degree centrality\"] = ''\n",
    "    \n",
    "    # Average Closeness centrality\n",
    "    closeness_centrality = nx.closeness_centrality(G)\n",
    "    try:\n",
    "        avg_closeness_centrality = sum(closeness_centrality.values()) / len(closeness_centrality)\n",
    "        statistics[\"Average closeness centrality\"] = avg_closeness_centrality\n",
    "    except:\n",
    "        statistics[\"Average closeness centrality\"] = ''\n",
    "    \n",
    "    # Average betweenness centrality\n",
    "    betweenness_centrality = nx.betweenness_centrality(G, weight = 'edge_cri')\n",
    "    try:\n",
    "        avg_betweenness_centrality = sum(betweenness_centrality.values()) / len(betweenness_centrality)\n",
    "        statistics[\"Average betweenness centrality\"] = avg_betweenness_centrality\n",
    "    except:\n",
    "        statistics[\"Average betweenness centrality\"] = ''\n",
    "\n",
    "#     # Average eigenvector centrality\n",
    "#     try:\n",
    "#         eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "#         avg_eigenvector_centrality = sum(eigenvector_centrality.values()) / len(eigenvector_centrality)\n",
    "#         statistics[\"Average eigenvector centrality\"] = avg_eigenvector_centrality\n",
    "#     else:\n",
    "#         statistics[\"Average eigenvector centrality\"] = 'Failed'\n",
    "        \n",
    "    # Graph modularity\n",
    "    communities = nx.algorithms.community.modularity_max.greedy_modularity_communities(G)\n",
    "    modularity = nx.algorithms.community.quality.modularity(G, communities)\n",
    "    statistics[\"Modularity Greedy\"] = modularity\n",
    "    avg_modularity = nx.algorithms.community.quality.modularity(G, communities) / G.number_of_nodes()\n",
    "    statistics[\"Norm. modularity Greedy\"] = avg_modularity\n",
    "    \n",
    "    # Louvain Modularity\n",
    "    communities = nx.algorithms.community.louvain_communities(G, weight = 'edge_cri',seed=123)\n",
    "    modularity = nx.algorithms.community.quality.modularity(G, communities)\n",
    "    statistics[\"Modularity Louvain\"] = modularity\n",
    "    avg_modularity = nx.algorithms.community.quality.modularity(G, communities) / G.number_of_nodes()\n",
    "    statistics[\"Norm. modularity Louvain\"] = avg_modularity\n",
    "    \n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_analysis_process(df, name):\n",
    "    \n",
    "    edgelist_vars = ['buyer_id','bidder_id','buyer_contracts', 'bidder_contracts', 'edge_contracts',\n",
    "             'buyer_cri', 'bidder_cri', 'edge_cri',\n",
    "             'buyer_tot_value', 'bidder_tot_value', 'edge_tot_value']\n",
    "    \n",
    "    # Process the data\n",
    "    df_processed = process_data(df)\n",
    "\n",
    "    # Create edge list\n",
    "    edgelist = df_processed[edgelist_vars]\n",
    "    edgelist = edgelist.sort_values(['buyer_id','bidder_id']).drop_duplicates(['buyer_id','bidder_id'], keep='first')\n",
    "\n",
    "    # Edgelist Statisitcs\n",
    "    c=pd.DataFrame(edgelist.describe())\n",
    "    c['graph'] = name\n",
    "\n",
    "    if os.path.isfile('data/desc_edgelist.csv'):\n",
    "        c.to_csv(f'data/desc_edgelist.csv', index = True, mode='a', header=False)\n",
    "    else:\n",
    "        c.to_csv(f'data/desc_edgelist.csv', index = True, mode='w', header=True)\n",
    "\n",
    "    # Create Graph\n",
    "    graph_statistics = []\n",
    "    G = create_graph(edgelist)\n",
    "    if G.number_of_nodes()>0:\n",
    "        graph_statistics.append(get_graph_statistics(G, name))\n",
    "\n",
    "    # Graph Statisitcs\n",
    "    graph_statistics_df = pd.DataFrame(graph_statistics)\n",
    "    graph_statistics_df = graph_statistics_df.drop_duplicates()\n",
    "    \n",
    "    if os.path.isfile('data/desc_graph.csv'):\n",
    "        graph_statistics_df.to_csv(f'data/desc_graph.csv', index = False, mode='a', header=False)\n",
    "    else:\n",
    "        graph_statistics_df.to_csv(f'data/desc_graph.csv', index = False, mode='w', header=True)\n",
    "        \n",
    "    \n",
    "    return df_processed, edgelist, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp_data = pp_data.sample(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Data Descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data if needed \n",
    "filter_vars = ['year','market','tender_supplytype','buyer_loc']\n",
    "\n",
    "# Runmain_analysis_process to get procurment data [contracts], modified edgelist, Graph object + Descriptive tables\n",
    "start = time.time()\n",
    "pp_data_global, edgelist_global, G_global = main_analysis_process(pp_data, 'Global')\n",
    "end = time.time()\n",
    "print(\"Time taken:\", end - start, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp_data = pp_data.sample(5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yearly outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_var = 'year'\n",
    "years = pp_data[filter_var].unique()\n",
    "\n",
    "start = time.time()\n",
    "for year in years:\n",
    "    pp_data_filtered = pp_data[pp_data[filter_var] == year]\n",
    "    pp_data_year, edgelist_year, G_year = main_analysis_process(pp_data_filtered, year)\n",
    "    exec(f\"pp_data_{filter_var}_{year} = pp_data_year\")\n",
    "    exec(f\"edgelist_{filter_var}_{year} = edgelist_year\")\n",
    "    exec(f\"G_{filter_var}_{year} = G_year\")\n",
    "    \n",
    "end = time.time()\n",
    "print(\"Time taken:\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years\n",
    "# G_year_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regional outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_var = 'buyer_loc'\n",
    "regions = pp_data[filter_var].unique()\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for region in regions:\n",
    "    pp_data_filtered = pp_data[pp_data[filter_var] == region]\n",
    "    pp_data_region, edgelist_region, G_region = main_analysis_process(pp_data_filtered, region)\n",
    "    exec(f\"pp_data_{filter_var}_{region} = pp_data_region\")\n",
    "    exec(f\"edgelist_{filter_var}_{region} = edgelist_region\")\n",
    "    exec(f\"G_{filter_var}_{region} = G_region\")\n",
    "end = time.time()\n",
    "print(\"Time taken:\", end - start, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save Graphs and edgelists on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write outputs to disk\n",
    "nx.write_graphml(G_global, 'data/graphs/G_global.graphml')\n",
    "edgelist_global.to_csv(f'data/edgelists/edgelist_global.csv', index = False, header=True)\n",
    "\n",
    "filter_var = 'year'\n",
    "years = pp_data[filter_var].unique()\n",
    "for year in years:\n",
    "    # Load the graph for the current year\n",
    "    G_year = eval(f'G_{filter_var}_{year}')\n",
    "    # Save the graph\n",
    "    nx.write_graphml(G_year, f'data/graphs/G_{filter_var}_{year}.graphml')\n",
    "    # Load the edgelist for the current year\n",
    "    edgelist_year = eval(f'edgelist_{filter_var}_{year}')\n",
    "    # Save the edgelist\n",
    "    edgelist_year.to_csv(f'data/edgelists/edgelist_{filter_var}_{year}.csv', index = False, header=True)\n",
    "\n",
    "filter_var = 'buyer_loc'\n",
    "regions = pp_data[filter_var].unique()\n",
    "for region in regions:\n",
    "    # Load the graph for the current year\n",
    "    G_region = eval(f'G_{filter_var}_{region}')\n",
    "    # Save the graph\n",
    "    nx.write_graphml(G_region, f'data/graphs/G_{filter_var}_{region}.graphml')\n",
    "    # Load the edgelist for the current region\n",
    "    edgelist_region = eval(f'edgelist_{filter_var}_{region}')\n",
    "    # Save the edgelist\n",
    "    edgelist_region.to_csv(f'data/edgelists/edgelist_{filter_var}_{region}.csv', index = False, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_graph = nx.read_graphml( f'data/graphs/G_buyer_loc_Severoistočen.graphml')\n",
    "\n",
    "    \n",
    "# if nx.is_isomorphic(G_buyer_loc_Severoistočen, loaded_graph):\n",
    "if nx.fast_could_be_isomorphic(G_buyer_loc_Severoistočen, loaded_graph):\n",
    "    print(\"The graphs are identical.\")\n",
    "else:\n",
    "    print(\"The graphs are not identical.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(G_str):\n",
    "    \n",
    "    # Graph name\n",
    "    G = eval(f'{G_str}')\n",
    "    name = G_str\n",
    "    print(name)\n",
    "    \n",
    "    # Largest connected component\n",
    "    Gc = max(nx.connected_components(G), key=len)\n",
    "    # Filter one degree nodes\n",
    "    Gc = nx.subgraph(G, [node for node in Gc if G.degree[node] > 1])\n",
    "\n",
    "    # # Filter out the edges between a range of cri values\n",
    "    Gc = nx.Graph(Gc)\n",
    "    edge_cri_values = [d['edge_cri'] for u,v,d in Gc.edges(data=True)]\n",
    "    first_quantile = np.quantile(edge_cri_values, 0.01)\n",
    "    last_quantile = np.quantile(edge_cri_values, 0.99)\n",
    "    Gc.remove_edges_from([(u,v) for (u,v,d) in Gc.edges(data=True) if (d['edge_cri'] < first_quantile or d['edge_cri'] > last_quantile)])\n",
    "\n",
    "    # Plotting \n",
    "    # for node, data in G.nodes(data=True):\n",
    "    #     print(f'Node: {node}')\n",
    "    #     for key, value in data.items():\n",
    "    #         print(f'\\t{key}: {value}')\n",
    "\n",
    "    # Node size = contract number \n",
    "    node_sizes = [d['contracts'] for n, d in Gc.nodes(data=True)]\n",
    "\n",
    "    # Node colors\n",
    "    source_nodes = [node for node in Gc.nodes() if node.startswith('B_')]\n",
    "    color_map = {}\n",
    "    for node in Gc.nodes():\n",
    "        if node in source_nodes:\n",
    "            color_map[node] = 'blue'\n",
    "        else:\n",
    "            color_map[node] = 'yellow'\n",
    "\n",
    "    # Edge Colors\n",
    "    edge_colors = [Gc[u][v]['edge_cri'] for u, v in Gc.edges()]\n",
    "\n",
    "    # Edge width - is edge edge_tot_value \n",
    "    edge_widths = [Gc[u][v]['edge_tot_value'] for u, v in Gc.edges()]\n",
    "    \n",
    "    # Layout\n",
    "    plt.figure(figsize=(12,12))\n",
    "    pos = nx.fruchterman_reingold_layout(Gc, scale=12, iterations=10, k=20, weight='edge_cri'*1000)\n",
    "\n",
    "    nx.draw_networkx_nodes(Gc, pos, node_size=node_sizes, node_color=[color_map[node] for node in Gc.nodes()], node_shape='o')\n",
    "    nx.draw_networkx_edges(Gc, pos, edge_color=edge_colors, edge_cmap=plt.cm.coolwarm, width=edge_widths)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.box(False)\n",
    "    # plt.show()\n",
    "    plt.savefig(f'data/figures/{name}.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_graph('G_buyer_loc_Severoistočen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot All graphs\n",
    "filter_vars = ['year','buyer_loc']\n",
    "for filter_var in filter_vars:\n",
    "    values = pp_data[filter_var].unique()\n",
    "    for value in values:\n",
    "#         print(f'G_{filter_var}_{value}')\n",
    "        plot_graph(f'G_{filter_var}_{value}')\n",
    "    \n",
    "plot_graph(f'G_global')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "# List of PNG files to load\n",
    "file_list = []\n",
    "for year in range(2011, 2023):\n",
    "    file = f'data/figures/G_year_{year}.png'\n",
    "    file_list.append(file)    \n",
    "    \n",
    "fig, axs = plt.subplots(4, 3, figsize=(100, 50))\n",
    "axs = axs.ravel()\n",
    "year = 2011\n",
    "for i, file in enumerate(file_list):\n",
    "    img = mpimg.imread(file)\n",
    "    axs[i].imshow(img)\n",
    "    axs[i].axis('off')\n",
    "    axs[i].set_title(f\"Year {year}\", fontsize=100)\n",
    "    year = year + 1\n",
    "fig.tight_layout()\n",
    "# plt.show()\n",
    "plt.savefig(f'data/figures/G_all_years.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# List of PNG files to load\n",
    "file_list = []\n",
    "regions = pp_data[filter_var].unique()\n",
    "for region in regions:\n",
    "    file = f'data/figures/G_buyer_loc_{region}.png'\n",
    "    file_list.append(file)    \n",
    "\n",
    "fig, axs = plt.subplots(2, 4, figsize=(100, 50))\n",
    "axs = axs.ravel()\n",
    "for i, file in enumerate(file_list):\n",
    "    region = file.replace('data/figures/G_buyer_loc_', '')\n",
    "    region = region.replace('.png', '')\n",
    "    img = mpimg.imread(file)\n",
    "    axs[i].imshow(img)\n",
    "    axs[i].axis('off')\n",
    "    axs[i].set_title(f\"{region}\", fontsize=100)\n",
    "fig.tight_layout()\n",
    "# plt.show()\n",
    "plt.savefig(f'data/figures/G_all_regions.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network region stats over time  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each region split over years and get stats to plot in a line plot \n",
    "\n",
    "regions = pp_data['buyer_loc'].unique()\n",
    "\n",
    "start = time.time()\n",
    "for region in regions:\n",
    "    pp_data_filtered_region = pp_data[pp_data['buyer_loc'] == region]\n",
    "    years = pp_data_filtered_region['year'].unique()\n",
    "    for year in years:\n",
    "        pp_data_filtered = pp_data_filtered_region[pp_data_filtered_region['year'] == f'{year}']\n",
    "        name  = f'{region}_{year}'\n",
    "#         print(name)\n",
    "#         print(len(pp_data_filtered))\n",
    "        pp_data_result, edgelist_result, G_result = main_analysis_process(pp_data_filtered, name)\n",
    "        exec(f\"pp_data_{region}_{year} = pp_data_result\")\n",
    "        exec(f\"edgelist_{region}_{year} = edgelist_result\")\n",
    "        exec(f\"G_{region}_{year} = G_result\")\n",
    "        \n",
    "        # Save the graph\n",
    "        nx.write_graphml(G_result, f'data/graphs/G_{region}_{year}.graphml')\n",
    "        # Save the edgelist\n",
    "        edgelist_result.to_csv(f'data/edgelists/edgelist_{region}_{year}.csv', index = False, header=True)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time taken:\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Region - Year line plots - Graph statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Figure A2.a: Regional Network Metrics Trend Analysis for North Macedonia: 2011-2022\n",
    "# Load graph descriptives\n",
    "desc = pd.read_csv(f'data/desc_graph.csv', encoding=\"utf-8\")\n",
    "desc = desc[desc['Name'].str.contains(r'_\d{4}', na=False)]\n",
    "desc[['region', 'year']] = desc['Name'].str.split('_', expand=True)\n",
    "desc['year'] = desc['year'].astype(int)\n",
    "# Convert \"year\" column to datetime format\n",
    "desc['year'] = pd.to_datetime(desc['year'], format='%Y')\n",
    "\n",
    "plots  = ['Number of nodes', 'Number of edges', 'Average degree',\n",
    "          'Assortativity', 'Average degree centrality','Average closeness centrality',\n",
    "          'Average betweenness centrality', 'Norm. modularity Greedy','Norm. modularity Louvain']\n",
    "\n",
    "for col in plots:\n",
    "    desc = desc.sort_values(by=\"year\")\n",
    "    fig, ax = plt.subplots()\n",
    "    for region in desc['region'].unique():\n",
    "        ax.plot(desc[desc['region'] == region]['year'], desc[desc['region'] == region][col], label=region)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel(col)\n",
    "    legend = ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=3)\n",
    "    plt.savefig(f'data/figures/Region_year_{col}.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(100, 50))\n",
    "file_list = []\n",
    "for col in plots:\n",
    "    file = f'data/figures/Region_year_{col}.png'\n",
    "    file_list.append(file)    \n",
    "    \n",
    "axs = axs.ravel()\n",
    "for i, file in enumerate(file_list):\n",
    "    img = mpimg.imread(file)\n",
    "    axs[i].imshow(img)\n",
    "    axs[i].axis('off')\n",
    "    axs[i].legend('off')\n",
    "    name = plots[i]\n",
    "    axs[i].set_title(name, fontsize=100)\n",
    "fig.tight_layout()\n",
    "# plt.show()\n",
    "plt.savefig(f'data/figures/Region_Year.png', dpi=300, bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cut off in May 2017 - when VMRO-DPMNE (incumbent) loses power \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter df based on cutoff \n",
    "pp_data['year'] = pp_data['year'].astype(int)\n",
    "pp_data['month'] = pp_data['month'].astype(int)\n",
    "\n",
    "pp_data['date'] = pd.to_datetime(pp_data['year'].astype(str) + '-' + pp_data['month'].astype(str), format='%Y-%m')\n",
    "\n",
    "# Before Period Jan 2012 - Apr 2017 (5 years)\n",
    "pp_data_before_long = pp_data[ (pp_data[\"year\"] >= 2012) & (pp_data[\"year\"] <= 2017) & (pp_data[\"date\"] < pd.to_datetime('2017-05-01'))]\n",
    "\n",
    "# After Period May 2017 - Dec 2022 (5 years)\n",
    "pp_data_after_long = pp_data[(pp_data[\"year\"] >= 2017) & (pp_data[\"date\"] >= pd.to_datetime('2017-05-01'))]\n",
    "\n",
    "# Before Period Jan 2015 - Apr 2017 (2 years)\n",
    "pp_data_before_short = pp_data[ (pp_data[\"year\"] >= 2015) & (pp_data[\"year\"] <= 2017) & (pp_data[\"date\"] < pd.to_datetime('2017-05-01'))]\n",
    "\n",
    "# After Period May 2017 - Dec 2019 (2 years)\n",
    "pp_data_after_short = pp_data[ (pp_data[\"year\"] >= 2017) & (pp_data[\"year\"] <= 2019) & (pp_data[\"date\"] >= pd.to_datetime('2017-05-01'))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = ['before', 'after']\n",
    "comparison = ['long','short']\n",
    "\n",
    "for t in time:\n",
    "    for comp in comparison:\n",
    "        print(t,comp)\n",
    "        exec(f\"pp_data_x = pp_data_{t}_{comp}\")\n",
    "        name = t+'_'+comp\n",
    "        pp_data_result, edgelist_result, G_result = main_analysis_process(pp_data_x, name)\n",
    "        nx.write_graphml(G_result, f'data/graphs/G_{t}_{comp}.graphml')\n",
    "        edgelist_result.to_csv(f'data/edgelists/edgelist_{t}_{comp}.csv', index = False, header=True)\n",
    "        exec(f\"pp_data_out_{t}_{comp} = pp_data_result\")\n",
    "        exec(f\"edgelist_{t}_{comp} = edgelist_result\")\n",
    "        exec(f\"G_{t}_{comp} = G_result\")\n",
    "        plot_graph(f'G_{t}_{comp}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The closeness centrality measures how quickly a node can reach all other nodes in the network. On the other hand, betweenness centrality measures how often a node appears on the shortest path between pairs of nodes.\n",
    "\n",
    "Based on the given information, we can conclude that the nodes in G1 are on average closer to all other nodes compared to the nodes in G2, while the nodes in G2 are on average more likely to be on the shortest path between pairs of nodes than the nodes in G1.\n",
    "\n",
    "The difference in average centrality values could indicate that the two graphs have different structures. For example, G1 may be a more densely connected network where nodes are generally close to each other, while G2 may have more long-range connections or hubs that are critical for connecting different parts of the network.\n",
    "\n",
    "In summary, the difference in average centrality values suggests that the two graphs have different properties and structures, and further analysis would be required to understand these differences in more detail.\n",
    "\n",
    "Global: This network has the highest number of nodes and edges compared to the other networks. It has an average degree of 10.21, which is the lowest among the networks. The assortativity coefficient is negative, indicating that nodes with high degree are more likely to be connected to nodes with low degree. The average degree centrality and closeness centrality values are the lowest among the networks, while the betweenness centrality value is the highest. The modularity values are also the highest among the networks, suggesting that the network can be divided into distinct communities or modules.\n",
    "\n",
    "before_long: This network has a lower number of nodes and edges compared to the global network. It has a slightly lower average degree of 8.45 and a negative assortativity coefficient, indicating that nodes with high degree are more likely to be connected to nodes with low degree. The average degree centrality and closeness centrality values are also slightly lower compared to the global network, while the betweenness centrality value is slightly higher. The modularity values are also lower than the global network, indicating that the network is less modular or has fewer distinct communities.\n",
    "\n",
    "after_long: This network has a lower number of nodes and edges compared to the global network but has a higher average degree of 13.5. It has a negative assortativity coefficient similar to the other networks. The average degree centrality, closeness centrality, and betweenness centrality values are all higher than the other networks, indicating that the nodes in this network are more central and important. The modularity values are slightly higher compared to the before_long network, suggesting that this network can be divided into more distinct communities.\n",
    "\n",
    "If one graph has higher degree and closeness centrality and lower betweenness centrality compared to another graph, it suggests that the first graph has a more centralized and densely connected structure compared to the second graph.\n",
    "Higher degree centrality implies that the nodes in the first graph have more direct connections to other nodes compared to the nodes in the second graph.\n",
    "Higher closeness centrality implies that the nodes in the first graph are closer, on average, to all other nodes in the graph compared to the nodes in the second graph.\n",
    "Lower betweenness centrality implies that the nodes in the first graph are less likely to act as bridges or bottlenecks between other nodes in the graph compared to the nodes in the second graph.\n",
    "\n",
    "If the first graph also has a higher modularity compared to the second graph, it suggests that the nodes in the first graph are more likely to form clusters or communities with high intra-cluster connectivity and low inter-cluster connectivity, compared to the nodes in the second graph.\n",
    "\n",
    "Modularity is a measure of the degree to which a network can be partitioned into subgroups or modules that are internally connected and have few connections between them. A higher modularity score suggests that the network is more modular or has more distinct subgroups. The modularity score can be calculated using various algorithms, including the greedy algorithm and Louvain algorithm.\n",
    "\n",
    "In combination with the higher degree and closeness centrality and lower betweenness centrality, a higher modularity score in the first graph suggests that it has a more hierarchical or structured organization, with distinct clusters or communities that are more densely connected internally and less connected to other clusters. This may indicate that the nodes in the first graph have more specialized roles or functions within their respective clusters, while the nodes in the second graph may be more heterogeneous in their connectivity and function. However, as with any analysis of complex networks, these interpretations are tentative and should be validated through further investigation and comparison with other measures and methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-test Before/After degree_centrality\n",
    "degree_centrality_before = nx.degree_centrality(G_before_long)\n",
    "avg_degree_centrality_before = sum(degree_centrality_before.values()) / len(degree_centrality_before)\n",
    "degree_centrality_after = nx.degree_centrality(G_after_long)\n",
    "avg_degree_centrality_after = sum(degree_centrality_after.values()) / len(degree_centrality_after)\n",
    "\n",
    "# Perform two-sample t-test for difference in modularity between G1 and G2\n",
    "t, p = stats.ttest_ind(list(degree_centrality_before.values()),list(degree_centrality_after.values()))\n",
    "\n",
    "# Print results\n",
    "print(\"t =\", t)\n",
    "print(\"p =\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-test Before/After closeness_centrality\n",
    "closeness_centrality_before = nx.closeness_centrality(G_before_long)\n",
    "closeness_centrality_after = nx.closeness_centrality(G_after_long)\n",
    "\n",
    "# Perform two-sample t-test for difference in modularity between G1 and G2\n",
    "t, p = stats.ttest_ind(list(closeness_centrality_before.values()),list(closeness_centrality_after.values()))\n",
    "\n",
    "# Print results\n",
    "print(\"t =\", t)\n",
    "print(\"p =\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-test Before/After  betweenness centrality\n",
    "betweenness_centrality_before = nx.betweenness_centrality(G_before_long, weight = 'edge_cri')\n",
    "betweenness_centrality_after = nx.betweenness_centrality(G_after_long, weight = 'edge_cri')\n",
    "\n",
    "# Perform two-sample t-test for difference in modularity between G1 and G2\n",
    "t, p = stats.ttest_ind(list(betweenness_centrality_before.values()),list(betweenness_centrality_after.values()))\n",
    "\n",
    "# Print results\n",
    "print(\"t =\", t)\n",
    "print(\"p =\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community detection algorithims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_community_info(edgelist, communities, algorithm):\n",
    "    # Create an empty list to hold the dataframe rows\n",
    "    rows = []\n",
    "\n",
    "    # Loop through the communities list and extract the community number and node id\n",
    "    for i, group in enumerate(communities):\n",
    "        for x in group:\n",
    "            rows.append([i, x])\n",
    "\n",
    "    # Create the dataframe\n",
    "    df = pd.DataFrame(rows, columns=['community_number', 'node_id'])\n",
    "\n",
    "    # Merge the community information to the edgelist\n",
    "    edgelist = pd.merge(edgelist, df, how='left', left_on='buyer_id', right_on='node_id')\n",
    "    edgelist = edgelist.rename(columns={'community_number': f'buyer_community_mod_{algorithm}'})\n",
    "    edgelist = edgelist.drop('node_id', axis=1)\n",
    "\n",
    "    edgelist = pd.merge(edgelist, df, how='left', left_on='bidder_id', right_on='node_id')\n",
    "    edgelist = edgelist.rename(columns={'community_number': f'bidder_community_mod_{algorithm}'})\n",
    "    edgelist = edgelist.drop('node_id', axis=1)\n",
    "\n",
    "    # Select the relevant columns and return the edgelist\n",
    "    return edgelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_types = ['global', 'before_long','after_long']\n",
    "algorithms = ['louvain','greedy']\n",
    "\n",
    "for graph_type in graph_types:\n",
    "    edgelist = pd.read_csv(f'data/edgelists/edgelist_{graph_type}.csv', encoding=\"utf-8\")\n",
    "    G = nx.read_graphml( f'data/graphs/G_{graph_type}.graphml')\n",
    "\n",
    "    # Run the community detection algorithm\n",
    "    for algorithm in algorithms:\n",
    "        print(f'Running the {algorithm} algorithm')\n",
    "        if algorithm == 'louvain':\n",
    "            communities = nx.algorithms.community.louvain_communities(G, weight = 'edge_cri',seed=123)\n",
    "        elif algorithm == 'greedy':\n",
    "            communities = nx.algorithms.community.modularity_max.greedy_modularity_communities(G)\n",
    "\n",
    "        # Merge the community information to the edgelist\n",
    "        print(f'Graph {graph_type}: The {algorithm} algorithm produced {len(communities)} communities' )\n",
    "        edgelist = add_community_info(edgelist, communities, algorithm)\n",
    "    \n",
    "    edgelist.to_csv(f'data/edgelists/edgelist_{graph_type}.csv', index = False, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By Supplier [Construction]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_data['buyer_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# БЕХТЕЛ & ЕНКА\n",
    "\n",
    "# Define the regular expression pattern in lower case\n",
    "pattern = 'ЕНКА'\n",
    "# pattern = 'Почетна'.lower()\n",
    "\n",
    "# Convert the 'buyer_name' column to lower case and filter the DataFrame by matching the pattern in a case-insensitive manner\n",
    "x = pp_data[pp_data['bidder_name'].str.lower().str.contains(pattern, flags=re.IGNORECASE)]\n",
    "\n",
    "x['bidder_name'].unique()\n",
    "x = x[['bidder_id','bidder_name']]\n",
    "x = x.drop_duplicates(subset=['bidder_id'])\n",
    "x['bidder_name'].unique()\n",
    "# print(x)\n",
    "# x.to_csv(f'data/processed/Networks/betton.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Betton construction company 2020  Beton Granit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define the regular expression pattern in lower case\n",
    "pattern = 'бетон'\n",
    "# pattern = 'Почетна'.lower()\n",
    "\n",
    "# Convert the 'buyer_name' column to lower case and filter the DataFrame by matching the pattern in a case-insensitive manner\n",
    "x = pp_data[pp_data['bidder_name'].str.lower().str.contains(pattern, flags=re.IGNORECASE)]\n",
    "\n",
    "x['bidder_name'].unique()\n",
    "x = x[['bidder_id','bidder_name']]\n",
    "x = x.drop_duplicates(subset=['bidder_id'])\n",
    "print(x)\n",
    "x.to_csv(f'data/betton.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Јавно претпријатие за државни патишта - Public enterprise for state roads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regular expression pattern in lower case\n",
    "pattern = r'\\b(?=\\w*државни)(?=\\w*патишта)\\w+\\b'\n",
    "pattern = r'патишта'\n",
    "\n",
    "# Convert the 'buyer_name' column to lower case and filter the DataFrame by matching the pattern in a case-insensitive manner\n",
    "x = pp_data[pp_data['buyer_name'].str.lower().str.contains(pattern, flags=re.IGNORECASE)]\n",
    "x = x[['buyer_id','buyer_name']]\n",
    "x = x.drop_duplicates(subset=['buyer_id'])\n",
    "print(x)\n",
    "x.to_csv(f'data/roads_buyer.csv', index = False, header=True)\n",
    "\n",
    "x = pp_data[pp_data['bidder_name'].str.lower().str.contains(pattern, flags=re.IGNORECASE)]\n",
    "x = x[['bidder_id','bidder_name']]\n",
    "x = x.drop_duplicates(subset=['bidder_id'])\n",
    "print(x)\n",
    "x.to_csv(f'data/roads_bidder.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beton - buyer's ego network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beton group of suppliers\n",
    "beton = ['S_895','S_1534','S_825','S_499','S_7759','S_1533','S_7583',\n",
    "         'S_824','S_857','S_856','S_826','S_896','S_827','S_500']\n",
    "\n",
    "# Return buyers in realtion with the beton group of suppliers\n",
    "beton_buyers = pp_data[pp_data['bidder_id'].isin(beton)]['buyer_id'].unique()\n",
    "\n",
    "# Dataset contians ego network all buyers in realtion with beton suppleirs\n",
    "pp_beton_full = pp_data.loc[pp_data['buyer_id'].isin(beton_buyers)]\n",
    "\n",
    "print(len(pp_beton_full['buyer_id'].unique()))\n",
    "print(len(pp_beton_full))\n",
    "\n",
    "#Filter df based on cutoff \n",
    "pp_beton_full['year'] = pp_beton_full['year'].astype(int)\n",
    "pp_beton_full['month'] = pp_beton_full['month'].astype(int)\n",
    "\n",
    "pp_beton_full['date'] = pd.to_datetime(pp_beton_full['year'].astype(str) + '-' + pp_beton_full['month'].astype(str), format='%Y-%m')\n",
    "\n",
    "# Before Period Jan 2012 - Apr 2017 (5 years)\n",
    "pp_beton_before = pp_beton_full[ (pp_beton_full[\"year\"] >= 2012) & (pp_beton_full[\"year\"] <= 2017) & (pp_beton_full[\"date\"] < pd.to_datetime('2017-05-01'))]\n",
    "\n",
    "# After Period May 2017 - Dec 2022 (5 years)\n",
    "pp_beton_after = pp_beton_full[(pp_beton_full[\"year\"] >= 2017) & (pp_beton_full[\"date\"] >= pd.to_datetime('2017-05-01'))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = ['full','before', 'after']\n",
    "\n",
    "for t in time:\n",
    "    exec(f\"pp_data_x = pp_beton_{t}\")\n",
    "    name = f'beton_{t}'\n",
    "    pp_data_result, edgelist_result, G_result = main_analysis_process(pp_data_x, name)\n",
    "    nx.write_graphml(G_result, f'data/graphs/G_beton_{t}.graphml')\n",
    "    edgelist_result.to_csv(f'data/edgelists/edgelist_beton_{t}.csv', index = False, header=True)\n",
    "    exec(f\"pp_data_out_beton_{t} = pp_data_result\")\n",
    "    exec(f\"edgelist_beton_{t} = edgelist_result\")\n",
    "    exec(f\"G_beton_{t} = G_result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roads as a buyer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roads_buyer = ['B_1191','B_1481','B_21']\n",
    "\n",
    "# Dataset contians ego network all buyers in realtion with beton suppleirs\n",
    "pp_roads_full = pp_data.loc[pp_data['buyer_id'].isin(roads_buyer)]\n",
    "\n",
    "print(len(pp_roads_full['buyer_id'].unique()))\n",
    "print(len(pp_roads_full))\n",
    "\n",
    "#Filter df based on cutoff \n",
    "pp_roads_full['year'] = pp_roads_full['year'].astype(int)\n",
    "pp_roads_full['month'] = pp_roads_full['month'].astype(int)\n",
    "\n",
    "pp_roads_full['date'] = pd.to_datetime(pp_roads_full['year'].astype(str) + '-' + pp_roads_full['month'].astype(str), format='%Y-%m')\n",
    "\n",
    "# Before Period Jan 2012 - Apr 2017 (5 years)\n",
    "pp_roads_before = pp_roads_full[ (pp_roads_full[\"year\"] >= 2012) & (pp_roads_full[\"year\"] <= 2017) & (pp_roads_full[\"date\"] < pd.to_datetime('2017-05-01'))]\n",
    "\n",
    "# After Period May 2017 - Dec 2022 (5 years)\n",
    "pp_roads_after = pp_roads_full[(pp_roads_full[\"year\"] >= 2017) & (pp_roads_full[\"date\"] >= pd.to_datetime('2017-05-01'))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = ['full','before', 'after']\n",
    "\n",
    "for t in time:\n",
    "    exec(f\"pp_data_x = pp_roads_{t}\")\n",
    "    name = f'roads_buyer_{t}'\n",
    "    pp_data_result, edgelist_result, G_result = main_analysis_process(pp_data_x, name)\n",
    "    nx.write_graphml(G_result, f'data/graphs/G_roads_buyer_{t}.graphml')\n",
    "    edgelist_result.to_csv(f'data/edgelists/edgelist_roads_buyer_{t}.csv', index = False, header=True)\n",
    "    exec(f\"pp_data_out_roads__buyer_{t} = pp_data_result\")\n",
    "    exec(f\"edgelist_roads__buyer_{t} = edgelist_result\")\n",
    "    exec(f\"G_roads__buyer_{t} = G_result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roads as a supplier - it's buyer's ego network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roads as a suppliers\n",
    "roads_supplier = ['S_11839']\n",
    "\n",
    "# Return buyers in realtion with the beton group of suppliers\n",
    "roads_supplier_buyers = pp_data[pp_data['bidder_id'].isin(roads_supplier)]['buyer_id'].unique()\n",
    "\n",
    "# Dataset contians ego network all buyers in realtion with beton suppleirs\n",
    "pp_roads_supplier_full = pp_data.loc[pp_data['buyer_id'].isin(roads_supplier_buyers)]\n",
    "\n",
    "print(len(pp_roads_supplier_full['buyer_id'].unique()))\n",
    "print(len(pp_roads_supplier_full))\n",
    "\n",
    "#Filter df based on cutoff \n",
    "pp_roads_supplier_full['year'] = pp_roads_supplier_full['year'].astype(int)\n",
    "pp_roads_supplier_full['month'] = pp_roads_supplier_full['month'].astype(int)\n",
    "\n",
    "pp_roads_supplier_full['date'] = pd.to_datetime(pp_roads_supplier_full['year'].astype(str) + '-' + pp_roads_supplier_full['month'].astype(str), format='%Y-%m')\n",
    "\n",
    "# Before Period Jan 2012 - Apr 2017 (5 years)\n",
    "pp_roads_supplier_before = pp_roads_supplier_full[ (pp_roads_supplier_full[\"year\"] >= 2012) & (pp_roads_supplier_full[\"year\"] <= 2017) & (pp_roads_supplier_full[\"date\"] < pd.to_datetime('2017-05-01'))]\n",
    "\n",
    "# After Period May 2017 - Dec 2022 (5 years)\n",
    "pp_roads_supplier_after = pp_roads_supplier_full[(pp_roads_supplier_full[\"year\"] >= 2017) & (pp_roads_supplier_full[\"date\"] >= pd.to_datetime('2017-05-01'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = ['full','before', 'after']\n",
    "\n",
    "for t in time:\n",
    "    exec(f\"pp_data_x = pp_roads_supplier_{t}\")\n",
    "    name = f'roads_supplier_{t}'\n",
    "    pp_data_result, edgelist_result, G_result = main_analysis_process(pp_data_x, name)\n",
    "    nx.write_graphml(G_result, f'data/graphs/G_roads_supplier_{t}.graphml')\n",
    "    edgelist_result.to_csv(f'data/edgelists/edgelist_roads_supplier_{t}.csv', index = False, header=True)\n",
    "    exec(f\"pp_data_out_roads_supplier_{t} = pp_data_result\")\n",
    "    exec(f\"edgelist_roads_supplier_{t} = edgelist_result\")\n",
    "    exec(f\"G_roads_supplier_{t} = G_result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Largest Construction Buyer B_133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_45_buyer = ['B_133']\n",
    "\n",
    "# Dataset contians ego network all buyers in realtion with beton suppleirs\n",
    "pp_large_45_full = pp_data.loc[pp_data['buyer_id'].isin(large_45_buyer)]\n",
    "\n",
    "print(len(pp_large_45_full['buyer_id'].unique()))\n",
    "print(len(pp_large_45_full))\n",
    "\n",
    "#Filter df based on cutoff \n",
    "pp_large_45_full['year'] = pp_large_45_full['year'].astype(int)\n",
    "pp_large_45_full['month'] = pp_large_45_full['month'].astype(int)\n",
    "\n",
    "pp_large_45_full['date'] = pd.to_datetime(pp_large_45_full['year'].astype(str) + '-' + pp_large_45_full['month'].astype(str), format='%Y-%m')\n",
    "\n",
    "# Before Period Jan 2012 - Apr 2017 (5 years)\n",
    "pp_large_45_before = pp_large_45_full[ (pp_large_45_full[\"year\"] >= 2012) & (pp_large_45_full[\"year\"] <= 2017) & (pp_large_45_full[\"date\"] < pd.to_datetime('2017-05-01'))]\n",
    "\n",
    "# After Period May 2017 - Dec 2022 (5 years)\n",
    "pp_large_45_after = pp_large_45_full[(pp_large_45_full[\"year\"] >= 2017) & (pp_large_45_full[\"date\"] >= pd.to_datetime('2017-05-01'))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pp_large_45_full)\n",
    "len(pp_large_45_before)+len(pp_large_45_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = ['full','before', 'after']\n",
    "\n",
    "for t in time:\n",
    "    exec(f\"pp_data_x = pp_large_45_{t}\")\n",
    "    name = f'large_45_buyer_{t}'\n",
    "    pp_data_result, edgelist_result, G_result = main_analysis_process(pp_data_x, name)\n",
    "    nx.write_graphml(G_result, f'data/graphs/G_large_45_buyer_{t}.graphml')\n",
    "    edgelist_result.to_csv(f'data/edgelists/edgelist_large_45_buyer_{t}.csv', index = False, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd largest construction buyer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_45_buyer = ['B_1191']\n",
    "\n",
    "# Dataset contians ego network all buyers in realtion with beton suppleirs\n",
    "pp_large_45_full = pp_data.loc[pp_data['buyer_id'].isin(large_45_buyer)]\n",
    "\n",
    "print(len(pp_large_45_full['buyer_id'].unique()))\n",
    "print(len(pp_large_45_full))\n",
    "\n",
    "#Filter df based on cutoff \n",
    "pp_large_45_full['year'] = pp_large_45_full['year'].astype(int)\n",
    "pp_large_45_full['month'] = pp_large_45_full['month'].astype(int)\n",
    "\n",
    "pp_large_45_full['date'] = pd.to_datetime(pp_large_45_full['year'].astype(str) + '-' + pp_large_45_full['month'].astype(str), format='%Y-%m')\n",
    "\n",
    "# Before Period Jan 2012 - Apr 2017 (5 years)\n",
    "pp_large_45_before = pp_large_45_full[ (pp_large_45_full[\"year\"] >= 2012) & (pp_large_45_full[\"year\"] <= 2017) & (pp_large_45_full[\"date\"] < pd.to_datetime('2017-05-01'))]\n",
    "\n",
    "# After Period May 2017 - Dec 2022 (5 years)\n",
    "pp_large_45_after = pp_large_45_full[(pp_large_45_full[\"year\"] >= 2017) & (pp_large_45_full[\"date\"] >= pd.to_datetime('2017-05-01'))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = ['full','before', 'after']\n",
    "\n",
    "for t in time:\n",
    "    exec(f\"pp_data_x = pp_large_45_{t}\")\n",
    "    name = f'large_45_2_buyer_{t}'\n",
    "    pp_data_result, edgelist_result, G_result = main_analysis_process(pp_data_x, name)\n",
    "    nx.write_graphml(G_result, f'data/graphs/G_large_45_2_buyer_{t}.graphml')\n",
    "    edgelist_result.to_csv(f'data/edgelists/edgelist_large_45_2_buyer_{t}.csv', index = False, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sectoral Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pp_data)\n",
    "pp_data['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp_data['market'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate market id\n",
    "pp_data['market_id'] = pp_data['tender_cpvs'].str[:2]\n",
    "pp_data['market_id'].unique()\n",
    "\n",
    "#Filter df based on cutoff \n",
    "pp_data['year'] = pp_data['year'].astype(int)\n",
    "pp_data['month'] = pp_data['month'].astype(int)\n",
    "pp_data['date'] = pd.to_datetime(pp_data['year'].astype(str) + '-' + pp_data['month'].astype(str), format='%Y-%m')\n",
    "#Filter out missing markets\n",
    "pp_data = pp_data[pp_data['market_id'] != '99']\n",
    "\n",
    "markets = pp_data['market_id'].unique()\n",
    "time = ['full','before', 'after']\n",
    "\n",
    "for market in markets:\n",
    "    print(f\"Market {market}\")\n",
    "    df = pp_data[pp_data['market_id'] == market]\n",
    "    for t in time:\n",
    "        if t == \"full\":\n",
    "            print(\"Full market\")\n",
    "            df_used = df\n",
    "        elif t == \"before\":\n",
    "            print(\"before market\")\n",
    "            df_used = df[ (df[\"year\"] >= 2012) & (df[\"year\"] <= 2017) & (df[\"date\"] < pd.to_datetime('2017-05-01'))]\n",
    "        else:\n",
    "            print(\"after market\")\n",
    "            df_used = df[(df[\"year\"] >= 2017) & (df[\"date\"] >= pd.to_datetime('2017-05-01'))]            \n",
    "        \n",
    "        name = f'market_{market}_{t}'\n",
    "        pp_data_result, edgelist_result, G_result = main_analysis_process(df_used, name)\n",
    "        nx.write_graphml(G_result, f'data/graphs/G_{market}_{t}.graphml')\n",
    "        edgelist_result.to_csv(f'data/edgelists/edgelist_{market}_{t}.csv', index = False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load graph descriptives\n",
    "desc = pd.read_csv(f'data/desc_graph.csv', encoding=\"utf-8\")\n",
    "desc = desc[desc['Name'].str.contains('market_', na=False)]\n",
    "desc[['scrap','market', 'time']] = desc['Name'].str.split('_', expand=True)\n",
    "desc = desc[desc['time'] != 'full']\n",
    "desc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for market_id mappings\n",
    "market_id_map = {\n",
    "    \"03\": \"Agricultural and related products\",\n",
    "    \"09\": \"Petroleum products and other sources of energy\",\n",
    "    \"14\": \"Mining related products\",\n",
    "    \"15\": \"Food, beverages and related products\",\n",
    "    \"16\": \"Agricultural machinery\",\n",
    "    \"18\": \"Clothing and accessories\",\n",
    "    \"19\": \"Leather and textile fabrics\",\n",
    "    \"22\": \"Printed matter and related products\",\n",
    "    \"24\": \"Chemical products\",\n",
    "    \"30\": \"Office and computing machinery\",\n",
    "    \"31\": \"Electrical machinery\",\n",
    "    \"32\": \"Telecommunication and related equipment\",\n",
    "    \"33\": \"Medical equipments and pharmaceuticals\",\n",
    "    \"34\": \"Transport equipment\",\n",
    "    \"35\": \"Security and defence equipment\",\n",
    "    \"37\": \"Musical instruments, sport goods, games, toys, handicraft, art materials and accessories\",\n",
    "    \"38\": \"Laboratory, optical and precision equipments (excl. glasses)\",\n",
    "    \"39\": \"Furniture (incl. office furniture), furnishings, domestic appliances (excl. lighting) and cleaning products\",\n",
    "    \"41\": \"Collected and purified water\",\n",
    "    \"42\": \"Industrial machinery\",\n",
    "    \"43\": \"Machinery for mining, quarrying, construction equipment\",\n",
    "    \"44\": \"Construction structures and materials; auxiliary products to construction (except electric apparatus)\",\n",
    "    \"45\": \"Construction work\",\n",
    "    \"48\": \"Software package and information systems\",\n",
    "    \"50\": \"Repair and maintenance services\",\n",
    "    \"51\": \"Installation services (except software)\",\n",
    "    \"55\": \"Hotel, restaurant and retail trade services\",\n",
    "    \"60\": \"Transport services\",\n",
    "    \"63\": \"Supporting and auxiliary transport services; travel agencies services\",\n",
    "    \"64\": \"Postal and telecommunications services\",\n",
    "    \"65\": \"Public utilities\",\n",
    "    \"66\": \"Financial and insurance services\",\n",
    "    \"70\": \"Real estate services\",\n",
    "    \"71\": \"Architectural, construction, engineering and inspection services\",\n",
    "    \"72\": \"IT services: consulting, software development, Internet and support\",\n",
    "    \"73\": \"Research and development services\",\n",
    "    \"75\": \"Administration, defence and social security services\",\n",
    "    \"76\": \"Services related to the oil and gas industry\",\n",
    "    \"77\": \"Agricultural services\",\n",
    "    \"79\": \"Business services: law, marketing, consulting, recruitment, printing and security\",\n",
    "    \"80\": \"Education and training services\",\n",
    "    \"85\": \"Health services\",\n",
    "    \"90\": \"Sewage, refuse, cleaning and environmental services\",\n",
    "    \"92\": \"Recreational services\",\n",
    "    \"98\": \"Other community, social and personal services\"\n",
    "}\n",
    "\n",
    "# Map the market_id to descriptive strings\n",
    "desc['market_str'] = desc['market'].map(market_id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desc['market'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols_to_remove = ['Name','scrap', 'market', 'time']\n",
    "plots = [element for element in desc.columns if element not in cols_to_remove]\n",
    "# plots = ['Average betweenness centrality']\n",
    "# desc = desc[desc['market'] != '33']\n",
    "\n",
    "plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure A2.b: Change in the number of nodes before and after the 2017 government change in North Macedonia\n",
    "x = desc[desc['market'].isin(['50', '09', '43', '85'])]\n",
    "plot = 'Number of nodes'\n",
    "\n",
    "pt = pd.pivot_table(x, values=plot, index='market_str', columns='time')\n",
    "pt['change'] = pt['after'] - pt['before']\n",
    "pt = pt.rename(columns={'change': plot})\n",
    "pt = pt[[plot]]\n",
    "\n",
    "# Sort the DataFrame by the values in the column\n",
    "pt = pt.sort_values(by=plot, ascending=True)  # Change to ascending=False for descending order\n",
    "\n",
    "# create a horizontal bar chart\n",
    "fig, ax = plt.subplots(figsize=(18, 12))  # set the figure size\n",
    "pt.plot.barh(legend=None, ax=ax)  # use horizontal bar chart\n",
    "\n",
    "# set the x and y axis labels\n",
    "ax.set_xlabel(f'Change in {plot}', labelpad=10, fontsize=20)  # x-axis label\n",
    "ax.set_ylabel('CPV Division', fontsize=29)  # y-axis label\n",
    "\n",
    "# control the font size of the tick labels (axis labels)\n",
    "ax.tick_params(axis='x', labelsize=17)  # x-axis tick labels\n",
    "ax.tick_params(axis='y', labelsize=22)  # y-axis tick labels\n",
    "\n",
    "# adjust the subplot spacing and margins\n",
    "plt.subplots_adjust(bottom=0.1, left=0.2, right=0.9, top=0.9)\n",
    "\n",
    "# save and show the chart\n",
    "plt.savefig(f'data/figures/Sector_{plot}.jpeg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure A2.c: Change in the number of distinct ties before and after the 2017 government change in North Macedonia\n",
    "# x = desc[desc['market'].isin(['33','50','43'])]\n",
    "x = desc[desc['market'].isin(['33','50','30','09','45'])]\n",
    "plot = 'Number of edges'\n",
    "\n",
    "pt = pd.pivot_table(x, values=plot, index='market_str', columns='time')\n",
    "pt['change'] = pt['after'] - pt['before']\n",
    "pt = pt.rename(columns={'change': plot})\n",
    "pt = pt[[plot]]\n",
    "\n",
    "# Sort the DataFrame by the values in the column\n",
    "pt = pt.sort_values(by=plot, ascending=True)  # Change to ascending=False for descending order\n",
    "\n",
    "# create a horizontal bar chart\n",
    "fig, ax = plt.subplots(figsize=(18, 12))  # set the figure size\n",
    "pt.plot.barh(legend=None, ax=ax)  # use horizontal bar chart\n",
    "\n",
    "# set the x and y axis labels\n",
    "ax.set_xlabel(f'Change in {plot}', labelpad=10, fontsize=20)  # x-axis label\n",
    "ax.set_ylabel('CPV Division', fontsize=29)  # y-axis label\n",
    "\n",
    "# control the font size of the tick labels (axis labels)\n",
    "ax.tick_params(axis='x', labelsize=17)  # x-axis tick labels\n",
    "ax.tick_params(axis='y', labelsize=22)  # y-axis tick labels\n",
    "\n",
    "# adjust the subplot spacing and margins\n",
    "plt.subplots_adjust(bottom=0.1, left=0.2, right=0.9, top=0.9)\n",
    "\n",
    "# save and show the chart\n",
    "plt.savefig(f'data/figures/Sector_{plot}.jpeg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure A2.d: Change in the average node degree before and after the 2017 government change in North Macedonia\n",
    "x = desc[desc['market'].isin(['33','30','09','50','71'])]\n",
    "plot = 'Average degree'\n",
    "\n",
    "pt = pd.pivot_table(x, values=plot, index='market_str', columns='time')\n",
    "pt['change'] = pt['after'] - pt['before']\n",
    "pt = pt.rename(columns={'change': plot})\n",
    "pt = pt[[plot]]\n",
    "\n",
    "# Sort the DataFrame by the values in the column\n",
    "pt = pt.sort_values(by=plot, ascending=True)  # Change to ascending=False for descending order\n",
    "\n",
    "# create a horizontal bar chart\n",
    "fig, ax = plt.subplots(figsize=(18, 12))  # set the figure size\n",
    "pt.plot.barh(legend=None, ax=ax)  # use horizontal bar chart\n",
    "\n",
    "# set the x and y axis labels\n",
    "ax.set_xlabel(f'Change in {plot}', labelpad=10, fontsize=20)  # x-axis label\n",
    "ax.set_ylabel('CPV Division', fontsize=29)  # y-axis label\n",
    "\n",
    "# control the font size of the tick labels (axis labels)\n",
    "ax.tick_params(axis='x', labelsize=17)  # x-axis tick labels\n",
    "ax.tick_params(axis='y', labelsize=22)  # y-axis tick labels\n",
    "\n",
    "# adjust the subplot spacing and margins\n",
    "plt.subplots_adjust(bottom=0.1, left=0.2, right=0.9, top=0.9)\n",
    "\n",
    "# save and show the chart\n",
    "plt.savefig(f'data/figures/Sector_{plot}.jpeg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure A2.e: Change in assortativity before and after the 2017 government change in North Macedonia\n",
    "x = desc[desc['market'].isin(['73','55','75','24'])]\n",
    "plot = 'Assortativity'\n",
    "\n",
    "pt = pd.pivot_table(x, values=plot, index='market_str', columns='time')\n",
    "pt['change'] = pt['after'] - pt['before']\n",
    "pt = pt.rename(columns={'change': plot})\n",
    "pt = pt[[plot]]\n",
    "\n",
    "# Sort the DataFrame by the values in the column\n",
    "pt = pt.sort_values(by=plot, ascending=True)  # Change to ascending=False for descending order\n",
    "\n",
    "# create a horizontal bar chart\n",
    "fig, ax = plt.subplots(figsize=(18, 12))  # set the figure size\n",
    "pt.plot.barh(legend=None, ax=ax)  # use horizontal bar chart\n",
    "\n",
    "# set the x and y axis labels\n",
    "ax.set_xlabel(f'Change in {plot}', labelpad=10, fontsize=20)  # x-axis label\n",
    "ax.set_ylabel('CPV Division', fontsize=29)  # y-axis label\n",
    "\n",
    "# control the font size of the tick labels (axis labels)\n",
    "ax.tick_params(axis='x', labelsize=17)  # x-axis tick labels\n",
    "ax.tick_params(axis='y', labelsize=22)  # y-axis tick labels\n",
    "\n",
    "# adjust the subplot spacing and margins\n",
    "plt.subplots_adjust(bottom=0.1, left=0.2, right=0.9, top=0.9)\n",
    "\n",
    "# save and show the chart\n",
    "plt.savefig(f'data/figures/Sector_{plot}.jpeg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure A2.f: Change in the average betweenness centrality before and after the 2017 government change in North Macedonia\n",
    "x = desc[desc['market'].isin(['64','16','19','73','09','14'])]\n",
    "# x = desc[(desc['market_str'] != 'Medical equipments and pharmaceuticals')] \n",
    "\n",
    "plot = 'Average betweenness centrality'\n",
    "\n",
    "pt = pd.pivot_table(x, values=plot, index='market_str', columns='time')\n",
    "pt['change'] = pt['after'] - pt['before']\n",
    "pt = pt.rename(columns={'change': plot})\n",
    "pt = pt[[plot]]\n",
    "\n",
    "# Sort the DataFrame by the values in the column\n",
    "pt = pt.sort_values(by=plot, ascending=True)  # Change to ascending=False for descending order\n",
    "\n",
    "# create a horizontal bar chart\n",
    "fig, ax = plt.subplots(figsize=(18, 12))  # set the figure size\n",
    "pt.plot.barh(legend=None, ax=ax)  # use horizontal bar chart\n",
    "\n",
    "# set the x and y axis labels\n",
    "ax.set_xlabel(f'Change in {plot}', labelpad=10, fontsize=20)  # x-axis label\n",
    "ax.set_ylabel('CPV Division', fontsize=29)  # y-axis label\n",
    "\n",
    "# control the font size of the tick labels (axis labels)\n",
    "ax.tick_params(axis='x', labelsize=17)  # x-axis tick labels\n",
    "ax.tick_params(axis='y', labelsize=22)  # y-axis tick labels\n",
    "\n",
    "# adjust the subplot spacing and margins\n",
    "plt.subplots_adjust(bottom=0.1, left=0.2, right=0.9, top=0.9)\n",
    "\n",
    "# save and show the chart\n",
    "plt.savefig(f'data/figures/Sector_{plot}.jpeg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure A2.g: Change in the Modularity score (Louvain algorithm) before and after the 2017 government change in North Macedonia\n",
    "x = desc[desc['market'].isin(['71','79','73','51'])]\n",
    "\n",
    "plot = 'Modularity Louvain'\n",
    "\n",
    "pt = pd.pivot_table(x, values=plot, index='market_str', columns='time')\n",
    "pt['change'] = pt['after'] - pt['before']\n",
    "pt = pt.rename(columns={'change': plot})\n",
    "pt = pt[[plot]]\n",
    "\n",
    "# Sort the DataFrame by the values in the column\n",
    "pt = pt.sort_values(by=plot, ascending=True)  # Change to ascending=False for descending order\n",
    "\n",
    "# create a horizontal bar chart\n",
    "fig, ax = plt.subplots(figsize=(18, 12))  # set the figure size\n",
    "pt.plot.barh(legend=None, ax=ax)  # use horizontal bar chart\n",
    "\n",
    "# set the x and y axis labels\n",
    "ax.set_xlabel(f'Change in {plot}', labelpad=10, fontsize=20)  # x-axis label\n",
    "ax.set_ylabel('CPV Division', fontsize=29)  # y-axis label\n",
    "\n",
    "# control the font size of the tick labels (axis labels)\n",
    "ax.tick_params(axis='x', labelsize=17)  # x-axis tick labels\n",
    "ax.tick_params(axis='y', labelsize=22)  # y-axis tick labels\n",
    "\n",
    "# adjust the subplot spacing and margins\n",
    "plt.subplots_adjust(bottom=0.1, left=0.2, right=0.9, top=0.9)\n",
    "\n",
    "# save and show the chart\n",
    "plt.savefig(f'data/figures/Sector_{plot}.jpeg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
